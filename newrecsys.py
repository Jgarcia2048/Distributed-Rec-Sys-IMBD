# -*- coding: utf-8 -*-
"""NewRecSys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1biSItJZC98Mj-J_mhJjhjDKA5SGIr_ry
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import sys
import time
import math 
import torch
import matplotlib
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import keras.models
import keras.layers
# from keras import models
# from keras import layers
from tensorflow.keras.datasets import imdb
import matplotlib.pyplot as plt
from __future__ import division
from collections import Counter
from __future__ import print_function
from tensorflow.keras.utils import to_categorical
from __future__ import absolute_import
from sklearn.model_selection import train_test_split

# import torch.distributed as dist
# import smdistributed.dataparallel.torch.torch_smddp

# dist.init_process_group(backend='smddp')

# (x_train, y_train), (x_test, y_test) = imdb.load_data(path="imdb.npz", seed=26)

# print(x_train.shape)
# print(y_train.shape)
# print(x_test.shape)
# print(y_test.shape)
# x_train[0:10]

def main(args):

  #Added
  def input_fn(eval):

    if eval == 0:
      x = train_x
      y = train_y
    else:
      x = test_x
      y = test_y

    x = tf.cast(x, tf.float32)
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(32)
    return dataset
  #-------------------------## 

  if len(args) < 2:
    print('You must specify model_dir for checkpoints such as'
          ' /tmp/tfkeras_example/.')
    return

  model_dir = args[1]
  print('Using %s to store checkpoints.' % model_dir)

  (training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)
  data = np.concatenate((training_data, testing_data), axis=0)
  targets = np.concatenate((training_targets, testing_targets), axis=0)

  def vectorize(sequences, dimension = 10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
      results[i, sequence] = 1
    return results
  
  data = vectorize(data)
  targets = np.array(targets).astype("float32")
  test_x = data[:10000]
  test_y = targets[:10000]
  train_x = data[10000:]
  train_y = targets[10000:]


  model = tf.keras.Sequential()
  # Input - Layer
  model.add(keras.layers.Dense(50, activation = "relu", input_shape=(10000, )))

  # Hidden - Layers
  model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))
  model.add(keras.layers.Dense(50, activation = "relu"))
  model.add(keras.layers.Dropout(0.2, noise_shape=None, seed=None))
  model.add(keras.layers.Dense(50, activation = "relu"))

  # Output- Layer
  model.add(keras.layers.Dense(1, activation = "sigmoid"))
  model.summary()

  # Compile the model.
  optimizer = tf.train.GradientDescentOptimizer(0.2)
  model.compile(loss='binary_crossentropy', optimizer=optimizer)
  model.summary()
  tf.keras.backend.set_learning_phase(True)

  results = model.fit(
  train_x, train_y,
  epochs= 2,
  batch_size = 500,
  validation_data = (test_x, test_y),
  use_multiprocessing=True
  )
  results.history


  #Added
  # Define DistributionStrategies and convert the Keras Model to an
  # Estimator that utilizes these DistributionStrateges.
  # Evaluator is a single worker, so using MirroredStrategy.
  config = tf.estimator.RunConfig(
      experimental_distribute=tf.contrib.distribute.DistributeConfig(
          train_distribute=tf.contrib.distribute.CollectiveAllReduceStrategy(
              num_gpus_per_worker=2),
          eval_distribute=tf.contrib.distribute.MirroredStrategy(
              num_gpus_per_worker=2)))
  keras_estimator = tf.keras.estimator.model_to_estimator(
      keras_model=model, config=config, model_dir=model_dir)
    
  # Train and evaluate the model. Evaluation will be skipped if there is not an
  # "evaluator" job in the cluster.
  tf.estimator.train_and_evaluate(
      keras_estimator,
      train_spec=tf.estimator.TrainSpec(input_fn=input_fn(0)),
      eval_spec=tf.estimator.EvalSpec(input_fn=input_fn(1)))

  #----------------------------------------------------------#

  if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run(argv=sys.argv)

